{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/damienbeneschi/mnist-eda-preprocessing-classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data in/ manipulating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data in\n",
    "test = pd.read_csv('/home/kilian/Desktop/Projects/test.csv')\n",
    "train = pd.read_csv('/home/kilian/Desktop/Projects/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting trainingset into labels and images\n",
    "train_y = train['label']\n",
    "train_x = train.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse pixel intensity\n",
    "print(train_x.describe())\n",
    "\n",
    "# visualize the distribution of the digits\n",
    "train_y.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_digit(N,train):\n",
    "    \"\"\"selects N randomn images out of the training set and displays the images with 8 images/row\"\"\"\n",
    "    # choose randomly N numbers out of 42000 and put them into a list\n",
    "    choose_N = np.random.randint(low=0,high=42000,size=N).tolist()   \n",
    "    # take a subset of N images\n",
    "    subset_images = train.iloc[choose_N,:]  \n",
    "    # reindex this subset of images\n",
    "    subset_images.index = range(1,N+1)  \n",
    "    # drop the lagel column out of the images-subset\n",
    "    subset_images_x = subset_images.drop(columns='label') \n",
    "    # iterate over the rows of the subset\n",
    "    for i, row in subset_images_x.iterrows():\n",
    "        # add subplot\n",
    "        plt.subplot((N//8)+1,8,i)\n",
    "        # reshape the images\n",
    "        pixels = row.values.reshape((28,28))\n",
    "        # display data as an image\n",
    "        plt.imshow(pixels, cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(\"{}\".format(subset_images.label.values[i-1]))\n",
    "    #plt.title(\"randomly picked images from the dataset\")\n",
    "    plt.show()\n",
    "    return \n",
    "          \n",
    "display_digit(20,train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_pixels(train_x):\n",
    "    \"\"\"removes pixels from the image with constant intensity values, hence which are always 0 (black) or 255 (white)\n",
    "       returns the cleared dataset and the list of removed pixels(columns)\"\"\"\n",
    "    changed_train_x = train_x.loc[:]\n",
    "    dropped_pixels_0 = []\n",
    "    # loop over all pixels (columns) in the image data set (train_x)\n",
    "    for col in train_x:\n",
    "        if changed_train_x[col].max() == 0:\n",
    "            changed_train_x.drop(columns=[col] , inplace=True)\n",
    "            dropped_pixels_0.append(col)  \n",
    "    dropped_pixels_255 = []\n",
    "    for col in changed_train_x:\n",
    "        if changed_train_x[col].min() == 255:\n",
    "            changed_train_x.drop(columns=[col] , inplace=True)\n",
    "            dropped_pixels_255.append(col)\n",
    "    return changed_train_x, dropped_pixels_0 + dropped_pixels_255\n",
    "\n",
    "print(train_x)\n",
    "changed_train_x, dropped_pixels = remove_constant_pixels(train_x)\n",
    "print(changed_train_x, \"\\n\", dropped_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pictures rescaling --> changing pixel values to either 0 or 255\n",
    "\n",
    "# pick a random index --> corresponding pixelvalues --> image\n",
    "index_random_picture = np.random.randint(low=0,high=42000, size=1).tolist()[0]\n",
    "pixels = train_x.iloc[index_random_picture,:]\n",
    "image = pixels.values.reshape((28,28))\n",
    "\n",
    "def rescaling_intensity(image):\n",
    "    \"\"\"rescales the image intensity and flattens it to one dimension\"\"\"\n",
    "    pmin, pmax = image.min(), image.max()\n",
    "    rescaled_image =255*(image-pmin) / (pmax - pmin)\n",
    "    rescaled_pixels = rescaled_image.flatten()\n",
    "    return rescaled_image, rescaled_pixels\n",
    "\n",
    "def black_white_only(pixels):\n",
    "    \"\"\"alters image and pixels to only white and black\"\"\"\n",
    "    only_bw_pixels = pixels.apply(lambda x : 0 if x<128 else 255)\n",
    "    only_bw_image = only_bw_pixels.values.reshape((28,28))\n",
    "    return only_bw_image, only_bw_pixels\n",
    "\n",
    "rescaled_image, rescaled_pixels = rescaling_intensity(image)\n",
    "only_bw_image, only_bw_pixels = black_white_only(pixels)\n",
    "\n",
    "# show images\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('original')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(rescaled_image, cmap='gray')\n",
    "plt.title('rescaled') # whereas the image is likely the same as the original, since the min and max intensity is 0 and 255\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(only_bw_image, cmap='gray')\n",
    "plt.title('black&white only image')\n",
    "plt.show()\n",
    "\n",
    "# pixel intesity histogram\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(pixels, bins=256, range=(0,256), density=True)\n",
    "plt.title('pixel dist - original image')\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(rescaled_pixels, bins=256, range=(0,256), density=True)\n",
    "plt.title('rescaled')\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(only_bw_pixels, bins=255, range=(0,256), density=True)\n",
    "plt.title(\"only black&white\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction methods\n",
    "Dimensionality reduction methods are used to lower the the dimensionality of the data by transforming the data from a high-dimensonal space into a low-dimensional space. \n",
    "By doing this that low-dimensional representation should remain most meaningful properties of the original data.\n",
    "Some of these methods are the principal components analysis (PCA) and non-negative matrix factorization (NMF).\n",
    "\n",
    "### Principal components analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction\n",
    "from sklearn.decomposition import PCA \n",
    "# Principal Components Analysis (PCA)\n",
    "samples = train.values\n",
    "digits=train.label.tolist()\n",
    "pca = PCA()\n",
    "pca.fit(samples)\n",
    "\n",
    "#PCA feature variance visualization\n",
    "pca_features =range(pca.n_components_)\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.bar(pca_features, pca.explained_variance_)\n",
    "plt.xticks(pca_features)\n",
    "plt.title('Principal Component Analysis for Dimension Reduction')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('Variance of the PCA feature')\n",
    "#_ = plt.savefig('visualizations/PCA features variance.png')\n",
    "plt.show()\n",
    "\n",
    "#PCA features variance visualization --> zoomed in \n",
    "# the first l features\n",
    "l=100\n",
    "x= range(l)\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.bar(x, pca.explained_variance_[:l])\n",
    "plt.xticks(x)\n",
    "plt.title('Principal Component Analysis for Dimension Reduction - zoomed in {} first features'.format(l))\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('Variance of the PCA feature')\n",
    "#_ = plt.savefig('visualizations/PCA features variance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "# visualization of the variance of the data carried by the number of PCA features\n",
    "n_components = np.array([1,2,3,4,5,6,10,30,60,80,100,200,400,700])\n",
    "cumul_variance = np.empty(len(n_components))\n",
    "for i, n in enumerate(n_components):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(samples)\n",
    "    cumul_variance[i]= np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "print(cumul_variance)\n",
    "\n",
    "_ = plt.figure(figsize=(30,20))\n",
    "_ = plt.grid(which='both')\n",
    "_ = plt.plot(n_components, cumul_variance, color='red')\n",
    "_ = plt.xscale('log')\n",
    "_ = plt.xlabel('number of PCA features', size=20)\n",
    "_ = plt.ylabel('cumulated variance of data(%)' , size = 20)\n",
    "_ = plt.title('data variance cumulated vs number of PCA features' , size=20)\n",
    "#plt.savefig('visualizations/cumulated variance_pca features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "s = np.random.randint(low=0, high=42001, size = 8200).tolist()\n",
    "samples = train.drop(columns='label').values\n",
    "\n",
    "nmf = NMF(n_components=16)\n",
    "nmf_features = nmf.fit_transform(samples)\n",
    "nmf_components = nmf.components_\n",
    "print('shape of NMF features: {}, shape of NMF components: {}'.format(nmf_features.shape, nmf_components.shape))\n",
    "\n",
    "#visualisation of the features\n",
    "for i, component in enumerate(nmf_components):\n",
    "    N = nmf_components.shape[0]\n",
    "    ax = plt.subplot((N//3)+1, 3, i+1)\n",
    "    bitmap = component.reshape((28,28))\n",
    "    plt.imshow(bitmap, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.title('NMF components from the original images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "#Sample randomly the dataset using discrete_uniform pick-up to reduce the amount of data\n",
    "#sample = np.random.randint(low=0, high=42001, size=8400).tolist()\n",
    "X = train.values  #.iloc[sample, :]\n",
    "X = X / 255.0\n",
    "y = train['label'].values  #.iloc[sample, :]\n",
    "print(\"Shape of X and Y arrays: {}\".format((X.shape, y.shape)))\n",
    "\n",
    "#Split the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4, stratify=y)\n",
    "\n",
    "#Yielding the scores according to number of NMF components\n",
    "components = np.arange(1, 100)\n",
    "scores = np.empty(len(components))\n",
    "for n in components:\n",
    "    pipeline = make_pipeline(NMF(n_components=n), SVC(kernel='rbf', cache_size=1000))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    scores[n-1] = pipeline.score(X_test, y_test)\n",
    "\n",
    "#Plotting of the scores\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.grid(which='both')\n",
    "plt.plot(components, scores)\n",
    "plt.xlabel('Number of NMF components', size=20)\n",
    "plt.ylabel('Score obtained', size=20)\n",
    "plt.title('Evolution of SVC classification score (samples={})'.format(len(y)), size=30)\n",
    "plt.savefig('visualizations/Score vs components NMF.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Best score {} obtained for {} components\".format(scores.max(), scores.argmax()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "\n",
    "#Sparsity visuaization & sparse matrix creation --> to reduce the memory size of the input\n",
    "samples = train.values\n",
    "_ = plt.figure(figsize=(10,100))\n",
    "_ = plt.spy(samples)\n",
    "plt.show()\n",
    "\n",
    "sparse_samples = csr_matrix(samples)\n",
    "\n",
    "#Memory Size comparison\n",
    "dense_size = samples.nbytes/1e6\n",
    "sparse_size = (sparse_samples.data.nbytes + \n",
    "               sparse_samples.indptr.nbytes + sparse_samples.indices.nbytes)/1e6\n",
    "print(\"From {} to {} Mo in memory usage with the sparse matrix\".format(dense_size, sparse_size))\n",
    "\n",
    "#Dimension reduction using PCA equivalent for sparse matrix\n",
    "model = TruncatedSVD(n_components=10)\n",
    "model.fit(sparse_samples)\n",
    "reduced_sparse_samples = model.transform(sparse_samples)\n",
    "print(reduced_sparse_samples.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning methods\n",
    "There are many machine learning methods, whereas the best fitting use cases alter from method to method.\n",
    "Some machine learning methods, which are promising for the purpose of digit recognition are presented below. \n",
    "These are: the k-nearest neighbors algorithm, linear discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors (KNN) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X and Y arrays: ((42000, 784), (42000,))\n",
      "Best accuracy on test set during training is 0.9657440476190476 obtained for {'n_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Prepare the X (features) and y (label) arrays for the sampled images\n",
    "sample = np.random.randint(low=0, high=42000, size=2100).tolist()\n",
    "X = train.iloc[:, 1:].values\n",
    "y = train.loc[:, 'label'].values \n",
    "print(\"Shape of X and Y arrays: {}\".format((X.shape, y.shape)))\n",
    "\n",
    "#Split the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4, stratify=y)\n",
    "\n",
    "#Fine tune the k value\n",
    "param_grid = {'n_neighbors': np.arange(1,10)}\n",
    "knn_cv = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "#Best k parameter\n",
    "best_k = knn_cv.best_params_\n",
    "best_accuracy = knn_cv.best_score_\n",
    "print(\"Best accuracy on test set during training is {} obtained for {}\".format(best_accuracy, best_k))\n",
    "#Best accuracy on test set during training is 0.9657440476190476 obtained for {'n_neighbors': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I got my till then my best accuracy of 0.9657440476190476 on the test set during training on the famous MNIST data obtained for n_neighbors = 5, I used KNN with n_neighbors=5 to predict the digits on the test set.\n",
    "I got a score of 0.96700 in the Digit Recognizer compitition on kaggle.\n",
    "Following is the code I obtained it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = train.iloc[:, 1:].values\n",
    "y = train.loc[:, 'label'].values \n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X, y)\n",
    "pred = knn.predict(test)\n",
    "\n",
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(pred)+1)), \"Label\": pred})\n",
    "submissions.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X and Y arrays: ((42000, 784), (42000,))\n",
      "Accuracy on test set: 0.8661904761904762\n",
      "Best accuracy during CV is 0.8706845238095238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X = train.iloc[:,1:].values \n",
    "y = train.loc[:, 'label'].values\n",
    "print(\"Shape of X and Y arrays: {}\".format((X.shape, y.shape)))\n",
    "\n",
    "#Split the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4, stratify=y)\n",
    "\n",
    "#Fit the model (no hyperparameter tuning for this model)\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto') #best results with these arguments\n",
    "#lda = QuadraticDiscriminantAnalysis()#reg_parameter=?\n",
    "lda.fit(X_train, y_train)\n",
    "score = lda.score(X_test, y_test)\n",
    "print(\"Accuracy on test set: {}\".format(score))\n",
    "\n",
    "#Best cv scores\n",
    "lda_cv_scores = cross_val_score(lda, X_train, y_train, cv=5)\n",
    "best_accuracy = lda_cv_scores.max()\n",
    "print(\"Best accuracy during CV is {}\".format(best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Prepare the X (features) and y (label) arrays for the sampled images\n",
    "sample = np.random.randint(low=0, high=42001, size=2100).tolist()\n",
    "X = train.iloc[sample, 1:].values\n",
    "y = train.loc[sample, 'label'].values#.reshape(-1,1)\n",
    "print(\"Shape of X and Y arrays: {}\".format((X.shape, y.shape)))\n",
    "\n",
    "#Split the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4, stratify=y)\n",
    "\n",
    "#Fine tune the hyperparameters using RandomizeSearchCV rather than GridSearchCV (too expensive with more than 1 hyperparameter)\n",
    "param_grid = {'C': np.logspace(0, 3, 20),\n",
    "              'gamma':np.logspace(0, -4, 20)#, not for linear kernel\n",
    "              #'degree': [2,3,4,5]  #only for poly kernel\n",
    "              #'coef0': []  #only for poly & sigmoid kernels\n",
    "             }\n",
    "svm_cv = RandomizedSearchCV(SVC(kernel='rbf', cache_size=3000), \n",
    "                            param_grid, cv=5)\n",
    "svm_cv.fit(X_train, y_train)\n",
    "\n",
    "#Best k parameter\n",
    "best_k = svm_cv.best_params_\n",
    "best_accuracy = svm_cv.best_score_\n",
    "print(\"Best accuracy on test set during training is {} obtained for {}\".format(best_accuracy, best_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Prepare the X (features) and y (label) arrays for the sampled images\n",
    "sample = np.random.randint(low=0, high=42000, size=4100).tolist()\n",
    "X = train.iloc[sample, 1:].values\n",
    "y = train.loc[sample, 'label'].values#.reshape(-1,1)\n",
    "print(\"Shape of X and Y arrays: {}\".format((X.shape, y.shape)))\n",
    "\n",
    "#Split the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4, stratify=y)\n",
    "\n",
    "#Fine tune the hyperparameters using RandomizeSearchCV rather than GridSearchCV (too expensive with more than 1 hyperparameter)\n",
    "param_grid = {'multi_class': ['ovr', 'crammer_singer'],\n",
    "              'penalty': ['l1', 'l2'],\n",
    "              'C': np.logspace(0, 4, 50)}\n",
    "\n",
    "linsvc_cv = GridSearchCV(LinearSVC(dual=False), param_grid, cv=5)\n",
    "#linsvc_cv = RandomizedSearchCV(LinearSVC(dual=False), param_grid, cv=5)\n",
    "linsvc_cv.fit(X_train, y_train)\n",
    "\n",
    "#Best k parameter\n",
    "best_k = linsvc_cv.best_params_\n",
    "best_accuracy = linsvc_cv.best_score_\n",
    "print(\"Best accuracy on test set during training is {} obtained for {}\".format(best_accuracy, best_k))\n",
    "#/home/kilian/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X and Y arrays: ((42000, 784), (42000,))\n",
      "Best accuracy on test set during training is 0.8488988095238096 obtained for {'min_samples_split': 7, 'min_samples_leaf': 8, 'max_depth': 36}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "X = train.values[:,1:]  \n",
    "y = train.loc[:, 'label'].values\n",
    "print(\"Shape of X and Y arrays: {}\".format((X.shape, y.shape)))\n",
    "\n",
    "#Split the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4, stratify=y)\n",
    "\n",
    "#Fine tune the hyperparameters\n",
    "param_grid = {'max_depth': np.arange(3,50),\n",
    "              'min_samples_leaf': np.arange(5,50),\n",
    "              'min_samples_split': np.arange(2,50)\n",
    "             }\n",
    "tree_cv = RandomizedSearchCV(DecisionTreeClassifier(),\n",
    "                       param_grid, cv=5)\n",
    "tree_cv.fit(X_train, y_train)\n",
    "\n",
    "#Best k parameter\n",
    "best_k = tree_cv.best_params_\n",
    "best_accuracy = tree_cv.best_score_\n",
    "print(\"Best accuracy on test set during training is {} obtained for {}\".format(best_accuracy, best_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Prepare the X (features) and y (label) arrays for the images\n",
    "X = csr_matrix(train.values)\n",
    "#X = X / 255.0  #intensities recaled between 0 and 1, NMF don't take negative values\n",
    "y = train['label'].values#.reshape(-1,1)   #idem if using sample\n",
    "print(\"Shape of X and Y arrays: {}\".format((X.shape, y.shape)))\n",
    "\n",
    "#Split the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4, stratify=y)\n",
    "\n",
    "#Pipeline with fine tune\n",
    "pipeline = Pipeline([#('scaler', StandardScaler()), \n",
    "                     ('pca', TruncatedSVD()),\n",
    "                     ('svm', SVC(kernel='rbf', cache_size=3000))\n",
    "                    ])\n",
    "param_grid = {'pca__n_components': np.arange(5, 80),\n",
    "              'svm__C': np.logspace(0, 4, 50),\n",
    "              'svm__gamma':np.logspace(0, -4, 50)}\n",
    "pipeline_cv = RandomizedSearchCV(pipeline, param_grid, cv=5)\n",
    "\n",
    "#fitting\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "#Best k parameter\n",
    "best_k = pipeline_cv.best_params_\n",
    "best_accuracy = pipeline_cv.best_score_\n",
    "print(\"Best accuracy on test set during training is {} obtained for {}\".format(best_accuracy, best_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "\n",
    "#Predict on the test dataset (holdout) that MUST contain as many columns (ie pixels) than in the training set\n",
    "holdout = pd.read_csv('test.csv').drop(columns=DROPPED_PIX)\n",
    "X_holdout = holdout.values\n",
    "print(X_holdout.shape)\n",
    "\n",
    "predictions = pipeline_cv.predict(X_holdout)\n",
    "submission_df = pd.DataFrame({'ImageId': range(1,28001), 'Label': predictions})\n",
    "print(\"Overview of the obtained predictions :\\n\", submission_df.head())\n",
    "\n",
    "#Save as submission file for competition\n",
    "submission_df.to_csv('submission_pca_svc_DB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a decision tree classifier with the best settings gotten from an randomized search on hyper parameters to predict the digits from the test dataset. the best accuracy on test set during training is 1.0 and was obtained for {'min_samples_split': 2, 'min_samples_leaf': 37, 'max_depth': 45}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8197619047619048\n",
      "[2 0 9 ... 3 9 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = train.values[:,1:]  \n",
    "y = train.loc[:, 'label'].values\n",
    "\n",
    "#Split the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4, stratify=y)\n",
    "\n",
    "tree = DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=37, max_depth=45)\n",
    "tree = tree.fit(X_train, y_train)\n",
    "prediction = tree.predict(X_test)\n",
    "\n",
    "a = accuracy_score(y_test, prediction)\n",
    "print(a)\n",
    "\n",
    "tree = DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=37, max_depth=45)\n",
    "tree = tree.fit(X, y)\n",
    "pred = tree.predict(test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
